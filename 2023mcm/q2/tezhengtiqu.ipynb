{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bee3941-c010-463f-9d84-734d35d091f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def consonant_cluster(word):\n",
    "    pattern = re.compile(r\"(bl|br|ch|cl|cr|dr|fl|fr|gl|gr|pl|pr|sc|sh|sk|sl|sm|sn|sp|st|sw|th|tr|tw|wh|wr)[aeiouy]\")\n",
    "    if pattern.search(word.lower()):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d54179d-f84e-4e6e-8a3c-dbf1183d6ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def vowel_cluster(word):\n",
    "    pattern = re.compile(r'[aeiou]{2,}')\n",
    "    match = re.search(pattern, word)\n",
    "    if match:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8207da7-f596-4df5-a5e1-40d3a683cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consonant_vowel_consonant(word):\n",
    "    # 判断单词长度是否小于3，如果是则直接返回 0\n",
    "    if len(word) < 3:\n",
    "        return 0\n",
    "    # 定义辅音和元音集合\n",
    "    consonants = set('bcdfghjklmnpqrstvwxyz')\n",
    "    vowels = set('aeiou')\n",
    "    # 循环遍历单词，判断是否存在辅音-元音-辅音的结构\n",
    "    for i in range(len(word)-2):\n",
    "        if (word[i] in consonants) and (word[i+1] in vowels) and (word[i+2] in consonants):\n",
    "            return 1\n",
    "    # 如果没有找到辅音-元音-辅音的结构，则返回 0\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d4b56f5-0683-404e-9427-64c06b70b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_duplicate_letters(word):\n",
    "    \"\"\"\n",
    "    计算单词中重复字母的个数\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    for letter in word:\n",
    "        if letter in counts:\n",
    "            counts[letter] += 1\n",
    "        else:\n",
    "            counts[letter] = 1\n",
    "    duplicates = sum(count for count in counts.values() if count > 1)\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea016186-c01d-4003-b5a6-aca81a433805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_duplicate_types(word):\n",
    "    \"\"\"\n",
    "    统计单词中重复字母的种类个数\n",
    "\n",
    "    参数：\n",
    "    word：str，待统计的单词\n",
    "\n",
    "    返回：\n",
    "    int：单词中重复字母的种类个数\n",
    "    \"\"\"\n",
    "    return len(set([letter for letter in word if word.count(letter) > 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25ca13c4-b42f-4dae-a8bb-1e6782f91baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eng_to_ipa\n",
    "\n",
    "def get_ipa(word):\n",
    "    \"\"\"\n",
    "    获取单词的国际音标形式\n",
    "    参数：\n",
    "        word：string，需要转换的单词\n",
    "    返回：\n",
    "        str：单词的国际音标形式\n",
    "    \"\"\"\n",
    "    return eng_to_ipa.convert(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6451bda2-074b-4655-836e-4eb0aac93959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ˈtrænˌskrɪpt'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ipa(\"transcript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1502920-1be0-4bc4-9347-cb7a40ef639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def has_diacritics(word):\n",
    "    # 匹配包含变音符号的字符\n",
    "    pattern = re.compile(r'[^\\u0000-\\u007F]')\n",
    "    # 判断单词中是否有变音符号\n",
    "    match = pattern.search(word)\n",
    "    if match:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "517cd119-bd28-4b80-8bd7-e6fb067dc809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import jellyfish\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "nltk.download(\"words\")\n",
    "def count_similar_words(word):\n",
    "    # 加载nltk中的词典\n",
    "    \n",
    "    word_list = words.words()\n",
    "    \n",
    "    # 选择五个字母的单词\n",
    "    similar_words = [w for w in word_list if len(w) == 5]\n",
    "    \n",
    "    # 计算与输入单词相似的单词数量\n",
    "    count = 0\n",
    "    for w in similar_words:\n",
    "        if jellyfish.jaro_winkler(word, w) > 0.9:\n",
    "            count += 1\n",
    "    return count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f9c800f-abde-4c36-a3cb-b9becaa58018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "import pandas as pd\n",
    "def word_frequency(word):\n",
    "    corpus = brown.words()\n",
    "    freq = corpus.count(word.lower())\n",
    "    total_words = len(corpus)\n",
    "    freq_percent = freq / total_words * 100\n",
    "    return freq_percent\n",
    "df = pd.read_csv('/root/2023mcm/Problem_C_Data_Wordle(1).csv', header=0)\n",
    "df[\"word_frequency_col\"] = df[\"Word\"].apply(word_frequency)\n",
    "df.to_csv('new_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d09b04e-50e7-4724-a072-3b415d19acb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'consonant_cluster' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/root/2023mcm/Problem_C_Data_Wordle(1).csv\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 在数据框中添加新列，对每个单词调用 consonant_cluster 函数\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsonant_cluster_col\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[43mconsonant_cluster\u001b[49m)\n\u001b[1;32m      8\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvowel_cluster_col\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(vowel_cluster)\n\u001b[1;32m      9\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsonant_vowel_consonant_col\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(consonant_vowel_consonant)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'consonant_cluster' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取 CSV 文件\n",
    "df = pd.read_csv('/root/2023mcm/Problem_C_Data_Wordle(1).csv', header=0)\n",
    "\n",
    "# 在数据框中添加新列，对每个单词调用 consonant_cluster 函数\n",
    "df[\"consonant_cluster_col\"] = df[\"Word\"].apply(consonant_cluster)\n",
    "df[\"vowel_cluster_col\"] = df[\"Word\"].apply(vowel_cluster)\n",
    "df[\"consonant_vowel_consonant_col\"] = df[\"Word\"].apply(consonant_vowel_consonant)\n",
    "df[\"count_duplicate_letters_col\"] = df[\"Word\"].apply(count_duplicate_letters)\n",
    "df[\"count_duplicate_types_col\"] = df[\"Word\"].apply(count_duplicate_types)\n",
    "df[\"get_phonemes_col\"] = df[\"Word\"].apply(get_ipa)\n",
    "df[\"has_diacritics_col\"] = df[\"get_phonemes_col\"].apply(has_diacritics)\n",
    "df[\"count_similar_words_col\"] = df[\"Word\"].apply(count_similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b996cfe8-6c37-42f4-bb0c-d441b5e906ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('new_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbb0878c-2374-4b07-aea0-b6b2242cfb07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 0, 3, 1, 'ˈɪri')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consonant_cluster(\"eerie\"),vowel_cluster(\"eerie\"),consonant_vowel_consonant(\"eerie\"),count_duplicate_letters(\"eerie\"),count_duplicate_types(\"eerie\"),get_ipa(\"eerie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1faa2836-184a-4ab3-a344-47702f2283bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_diacritics('ˈɪri'),count_similar_words(\"eerie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e2cc4-1af9-49b8-9b4f-465ab504270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_frequency_by_position(df, position):\n",
    "    # 先将单词中的字母拆开成列表，然后根据字母所在位置转换成数据框形式\n",
    "    df_letter = pd.DataFrame(df[\"Word\"].apply(lambda x: list(x)))\n",
    "    df_letter[position] = df_letter[0].apply(lambda x: x[position - 1] if len(x) >= position else np.nan)\n",
    "    df_letter = df_letter.explode(position)\n",
    "    df_letter.columns = [\"letter\", \"position\"]\n",
    "    df_letter.dropna(inplace=True)\n",
    "    # 计算字母频率\n",
    "    df_letter_counts = df_letter.groupby([\"position\", \"letter\"]).size().reset_index(name=\"count\")\n",
    "    df_letter_counts[\"frequency\"] = df_letter_counts[\"count\"] / len(df_letter)\n",
    "    # 按照频率排序\n",
    "    df_letter_counts = df_letter_counts.sort_values(by=[\"position\", \"frequency\"], ascending=[True, False])\n",
    "    return df_letter_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd501479-4a61-4815-b868-2be100c8850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/root/2023mcm/new_data.csv\")\n",
    "letter_frequency_by_position(data,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58cfd55-e877-4a64-b593-d1474f8f9de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec514b2-95b6-4661-8f24-448f70ec3a03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
